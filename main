Thank you for your interest 

Submitted Journal: Neurocomputing
If the manuscript is accepted, the source code will be opened immediately.

Cross-modal visible-infrared person re-identification (VI-ReID), which is designed to retrieve pedestrian images captured by visible and infrared cameras, represents a challenging but necessary task for intelligent surveillance systems. The large style differences between visible and infrared images lead to intra-class differences in the same pedestrian. Most current methods address the large variability between cross-modal images by feature generation and fine-grained feature mining. However, these methods are not only costly but also ignore the correlation between the same pedestrian in two modalities. The effect of image style differences and feature variations on model performance can be mitigated by some external strategies. Therefore, in this paper, we propose a novel spatial-instance normalization module to mitigate the style differences and a unique cross-modal feature reorganization operation that drives the model to be more robust to minor changes in intra-class features. In addition, we propose a new character loss function that aims to capture more diverse features of the same pedestrian in different modalities. The experimental results on two public datasets show that our method is competitive with the SOTA method.
